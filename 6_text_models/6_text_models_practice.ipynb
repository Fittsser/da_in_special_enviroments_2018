{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Acknowledgements. Данное занятие в значительной степени полагается на материалы семинара Антона Михайловича Алексеева, который он провёл на Летней школе 2018 года.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/автостопом.txt\", encoding=\"cp1251\") as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Посвящается Джонни Броку, Клэр Горст и всем остальным арглингтонийцам — в благодарность за чай, сочувствие и диван.\\n\\n\\nДалеко-далеко, в не замеченных картографами складках давно вышедшего из моды Запад'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "BOS = \"BOS\"\n",
    "EOS = \"EOS\"\n",
    "UNK = \"UNK\"\n",
    "\n",
    "def prepare_sentences(txt, word_threshold=2, stage_train=True):\n",
    "    \n",
    "    # заменяем многие пробелы на один\n",
    "    txt = re.sub(\"\\s+\", \" \", txt) \\\n",
    "        .lower() # и переводим в нижний регистр\n",
    "    \n",
    "    # заменяем символы переноса строки на пробелы (абзацы нам не нужны)\n",
    "    text = txt.replace(\"\\n\", \" \")\n",
    "    # разбиваем текст на предложения по знакам препинания\n",
    "    sentences = re.split(\"[!\\?\\.]+\", txt.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    # оставляем только alphanumeric\n",
    "    # W — любой символ, кроме буквенного или цифрового символа или знака подчёркивания\n",
    "    clean_sentences = [re.split(\"\\W+\", s) for s in sentences]\n",
    "    \n",
    "    # заменяем числа на NUM\n",
    "    clean_sentences = [[w.replace(\"\\d+\", \"NUM\") for w in s if w] for s in clean_sentences]\n",
    "    \n",
    "    # вводим тег UNKNOWN: UNK\n",
    "    if stage_train:\n",
    "\n",
    "        counter = Counter()\n",
    "\n",
    "        for s in clean_sentences:\n",
    "            for w in s:\n",
    "                counter[w] += 1\n",
    "    \n",
    "        print(\"Filtered out word types :\", len([w for w in counter if counter[w] <= word_threshold]))\n",
    "        print(\"Filtered out words count:\", sum([counter[w] for w in counter if counter[w] <= word_threshold]))\n",
    "    \n",
    "        # выкидываем редкие, и заменяем их на специальный тег\n",
    "        clean_sentences = [[w if counter[w] > word_threshold else UNK for w in s] for s in clean_sentences]            \n",
    "    \n",
    "    word2index = { BOS: 0, EOS: 1, UNK: 2}\n",
    "    index2word = { 0: BOS, 1: EOS, 2: UNK}\n",
    "    \n",
    "    counter = max(word2index.values()) + 1\n",
    "\n",
    "    for s in clean_sentences:\n",
    "        for w in s:\n",
    "            if not w in word2index:\n",
    "                word2index[w] = counter\n",
    "                index2word[counter] = w\n",
    "                counter += 1\n",
    "                \n",
    "    return word2index, index2word, clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out word types : 34282\n",
      "Filtered out words count: 41806\n",
      "Total number of sentences :\t 25610\n",
      "Total number of words     :\t 275830\n",
      "Total number of word types:\t 12238\n"
     ]
    }
   ],
   "source": [
    "word2index, index2word, clean_sentences = prepare_sentences(txt)\n",
    "\n",
    "print(\"Total number of sentences :\\t\", len(clean_sentences))\n",
    "print(\"Total number of words     :\\t\", sum([len(sent) for sent in clean_sentences]))\n",
    "print(\"Total number of word types:\\t\", len(set([w for sent in clean_sentences for w in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(sentence, context_size):\n",
    "    \"\"\"\n",
    "        Добиваем символы начала и конца строки к каждому предложению\n",
    "    \"\"\"\n",
    "    return [BOS] * context_size + sentence + [EOS] * context_size\n",
    "\n",
    "def enumerate_sentences(clean_sentences, context_size, word2index):\n",
    "    \"\"\"\n",
    "        Добиваем символами начала и конца и конвертируем слова в индексы\n",
    "    \"\"\"\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    UNK_id = word2index[UNK]\n",
    "\n",
    "    for sentence in clean_sentences:\n",
    "\n",
    "        aligned_sentence =  augment(sentence, context_size) \n",
    "\n",
    "        for i in range(context_size, len(sentence) - context_size, 1):\n",
    "            \n",
    "            # берём предшествующий контекст\n",
    "            context = aligned_sentence[i - context_size:i]\n",
    "            context = [word2index[c] if c in word2index else UNK_id for c in context]\n",
    "            target = word2index[aligned_sentence[i]] if aligned_sentence[i] in word2index else UNK_id\n",
    "            \n",
    "            contexts.append(context)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l0, l1, n):\n",
    "    \n",
    "    assert len(l0) == len(l1)\n",
    "    coll0, coll1 = [], []\n",
    "    \n",
    "    for i in range(0, len(l0), n):\n",
    "        coll0.append(l0[i:i + n])\n",
    "        coll1.append(l1[i:i + n])\n",
    "        \n",
    "    return coll0, coll1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import lru_cache\n",
    "\n",
    "class NGramFreqsLanguageModeler(object):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_size):\n",
    "        super(NGramFreqsLanguageModeler, self).__init__()\n",
    "    \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.ngram_dict = defaultdict(lambda: defaultdict(lambda: 0))        \n",
    "        self.n_1_gram_dict = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.contexts_counts = defaultdict(lambda: 0)\n",
    "        self.eps = 1.0\n",
    "    \n",
    "    def fit(self, contexts, targets):\n",
    "        \n",
    "        self.contexts_counts = defaultdict(lambda: 0)\n",
    "        \n",
    "        for c, t in zip(contexts, targets):\n",
    "            c = tuple(c)\n",
    "            self.ngram_dict[c][t] += 1\n",
    "            self.contexts_counts[c] += 1\n",
    "            \n",
    "            # намёк!\n",
    "            # self.n_1_gram_dict[c[1:]][t] += 1\n",
    "\n",
    "            \n",
    "        print(\"Total n-1 grams\", len(self.ngram_dict), list(self.ngram_dict)[:10])\n",
    "        \n",
    "        # нормализуем частоты\n",
    "        for c in tqdm_notebook(self.ngram_dict.keys()):\n",
    "            for t in self.ngram_dict[c]:\n",
    "                self.ngram_dict[c][t] = (self.ngram_dict[c][t] +  self.eps) / \\\n",
    "                                            (self.contexts_counts[c] + self.vocab_size * self.eps)\n",
    "        \n",
    "    @lru_cache(1000000)\n",
    "    def prob_dist(self, input_context):\n",
    "        \"\"\"\n",
    "            Takes ngram as a tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = np.zeros(self.vocab_size) + \\\n",
    "                    self.eps / (self.vocab_size * self.eps + self.contexts_counts[input_context])\n",
    "        \n",
    "        counts = self.ngram_dict[input_context]\n",
    "        \n",
    "        # если есть хоть какие-то счётчики\n",
    "        if counts:\n",
    "            \n",
    "            # проставим осмысленные частоты\n",
    "            for target, freq in counts.items():\n",
    "                probs[target] = freq\n",
    "                \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "BATCH_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n-1 grams 101108 [(0, 0, 0), (0, 0, 3), (0, 3, 4), (3, 4, 2), (4, 2, 2), (2, 2, 2), (2, 2, 5), (2, 5, 6), (5, 6, 7), (6, 7, 2)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b338efdd558a4a518f77910aaeb0cbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=101108), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "\n",
    "# строим контексты и цели\n",
    "contexts, targets = enumerate_sentences(clean_sentences, CONTEXT_SIZE, word2index)\n",
    "\n",
    "batches = list(zip(contexts, targets))\n",
    "\n",
    "simple_model = NGramFreqsLanguageModeler(context_size=CONTEXT_SIZE, vocab_size=len(word2index))\n",
    "simple_model.fit(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерация: EOS который\n",
      "Генерация: EOS который ночей\n",
      "Генерация: EOS который ночей блестящий\n",
      "Генерация: EOS который ночей блестящий доброго\n",
      "Генерация: EOS который ночей блестящий доброго мистер\n",
      "Генерация: EOS который ночей блестящий доброго мистер кататься\n",
      "Генерация: EOS который ночей блестящий доброго мистер кататься алфавит\n",
      "Генерация: EOS который ночей блестящий доброго мистер кататься алфавит ярдах\n",
      "Генерация: EOS который ночей блестящий доброго мистер кататься алфавит ярдах странности\n",
      "Генерация: EOS который ночей блестящий доброго мистер кататься алфавит ярдах странности отбросил\n"
     ]
    }
   ],
   "source": [
    "test = \"BOS\"\n",
    "prepared_text = augment(prepare_sentences(test, stage_train=False)[2][0], CONTEXT_SIZE)[-CONTEXT_SIZE:]\n",
    "\n",
    "for i in range(CONTEXT_SIZE, 10 + CONTEXT_SIZE):\n",
    "    \n",
    "    idx = [word2index[w] for w in prepared_text[:i]]    \n",
    "    \n",
    "    predict = simple_model.prob_dist(tuple(idx[-CONTEXT_SIZE:])) \n",
    "    \n",
    "#     predict = predict - predict.min()  \n",
    "#     predict /= sum(predict)\n",
    "    \n",
    "    selected_word = np.random.choice(a=list(range(len(word2index))), p=predict)    \n",
    "    prepared_text.append(index2word[selected_word])\n",
    "    \n",
    "    print(\"Генерация:\", \" \".join(prepared_text[CONTEXT_SIZE - 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences :\t 4818\n",
      "Total number of words     :\t 45413\n",
      "Total number of word types:\t 11278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22410, 22410, 11, 12240)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/ресторан.txt\") as f:\n",
    "    test_txt = f.read()\n",
    "\n",
    "_, _, test_clean_sentences = prepare_sentences(test_txt, stage_train=False)\n",
    "\n",
    "print(\"Total number of sentences :\\t\", len(test_clean_sentences))\n",
    "print(\"Total number of words     :\\t\", sum([len(sent) for sent in test_clean_sentences]))\n",
    "print(\"Total number of word types:\\t\", len(set([w for sent in test_clean_sentences for w in sent])))\n",
    "\n",
    "# строим контексты и цели\n",
    "test_contexts, test_targets = enumerate_sentences(test_clean_sentences, CONTEXT_SIZE, word2index)\n",
    "\n",
    "# test_data = list(zip(test_contexts, test_targets))\n",
    "\n",
    "test_batched_contexts, test_batched_targets = chunks(test_contexts, test_targets, BATCH_SIZE)\n",
    "test_batches = list(zip(test_batched_contexts, test_batched_targets))\n",
    "\n",
    "len(test_contexts), len(test_targets), len(test_batches), len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def compute_ppl_count_model(model, test_batches, loss_function):\n",
    "    \n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for context_batch, target_batch in tqdm_notebook(test_batches):\n",
    "        \n",
    "        log_probs = []\n",
    "        \n",
    "        for context, target in zip(context_batch, target_batch):\n",
    "            \n",
    "            # применяем модель\n",
    "            log_probs.append(np.log2(model.prob_dist(tuple(context))))\n",
    "            \n",
    "        log_probs = np.array(log_probs)\n",
    "        \n",
    "        # вычисляем невязку\n",
    "        loss = loss_function(torch.tensor(log_probs, dtype=torch.float), \n",
    "                       torch.tensor(target_batch, dtype=torch.long))\n",
    "        \n",
    "        # получаем число\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        \n",
    "        if count % (len(test_batches) // 5) == 0:\n",
    "            print(count, \"\\tloss\", total_loss)\n",
    "            print([index2word[i] for i in context_batch[0]], \"->\", \n",
    "                  index2word[target_batch[0]], \"vs\", \n",
    "                  [index2word[i] for i in (-log_probs[0]).argsort()[:3]],\n",
    "                  np.sort((-log_probs[0]))[:3]\n",
    "                 )\n",
    "    \n",
    "    return 2 ** (total_loss / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12366a0c797b4a198717102938c91351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \tloss 17.510966300964355\n",
      "['смотрела', 'прямо', 'на'] -> них vs ['BOS', 'здоровой', 'возбуждение'] [13.57931594 13.57931594 13.57931594]\n",
      "4 \tloss 34.798720359802246\n",
      "['обращается', 'голос', 'UNK'] -> хозяина vs ['BOS', 'здоровой', 'возбуждение'] [13.57931594 13.57931594 13.57931594]\n",
      "6 \tloss 52.32221984863281\n",
      "['во', 'время', 'UNK'] -> играют vs ['посадки', 'UNK', 'так'] [12.57990515 12.57990515 12.57990515]\n",
      "8 \tloss 69.98028659820557\n",
      "['в', 'атмосфере', 'и'] -> все vs ['BOS', 'здоровой', 'возбуждение'] [13.57931594 13.57931594 13.57931594]\n",
      "10 \tloss 87.3246488571167\n",
      "['UNK', 'гладь', 'UNK'] -> по vs ['BOS', 'здоровой', 'возбуждение'] [13.57931594 13.57931594 13.57931594]\n",
      "\n",
      "Perplexity of freq-based NGram model on test set 426.5119103413951\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "print(\"Perplexity of freq-based NGram model on test set\", compute_ppl_count_model(model=simple_model, \n",
    "                                                                            loss_function=CrossEntropyLoss(),\n",
    "                                                                            test_batches=test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задача\n",
    "Требуется реализовать языковую модель, с помощью которой можно решать задачу отделения \"естественных\" текстов от сгенерированных -- с использованием предложенного обучающего множества.\n",
    "\n",
    "Перед вами список пар текстов, и требуется сказать, какой из них \"настоящий\"; 1 — первый, 0 — второй. Оценка качества алгоритма -- accuracy, то есть доля истинных попаданий.\n",
    "\n",
    "Модель обучается на `train.tsv`. Модель применяется к `test.tsv`.\n",
    "\n",
    "Истинные метки можно найти в файле `true_labels.tsv`.\n",
    "\n",
    "Можно использовать вышеопределённую модель или любую другую."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
